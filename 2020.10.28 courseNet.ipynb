{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sklearn, keras, tensorflow, gensim, multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential, Model\n",
    "from keras import layers\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, Input, Embedding, BatchNormalization\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set working directory\n",
    "dirname = ''\n",
    "os.chdir(dirname)\n",
    "\n",
    "# Read course description data\n",
    "X = read_csv('input/courses_nested.csv')\n",
    "\n",
    "# DEFINE CONSTANTS\n",
    "N = 5000           # Number of words in document matrix\n",
    "TEST_SIZE = 0.05   # Define split size for test, training data (% of all records)\n",
    "LAM = 0.01        # Regularization parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process outcome data y\n",
    "le = LabelEncoder() \n",
    "onehot_encoder = OneHotEncoder(sparse = False)\n",
    "\n",
    "y_v2 = le.fit_transform(np.array(X['blom_group']))\n",
    "y_v2 = y_v2.reshape(len(y_v2), 1)\n",
    "y_onehot = onehot_encoder.fit_transform(y_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count vectorizer for features\n",
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(max_features = N, ngram_range = (1,2))  # ALLOW LARGER NGRAM: ngram_range = (1,2)?\n",
    "x = vectorizer.fit_transform(X['desc'])\n",
    "x_dense = x.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# PREPARE DATASETS FOR LEARING\n",
    "##########################################\n",
    "\n",
    "# EVENTUALLY - NEED TO NORMALIZE THE X DATA\n",
    "\n",
    "# Split into test, training datasets\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(x_dense, y_onehot, test_size = TEST_SIZE, shuffle = True, stratify = y_onehot)\n",
    "train_X, dev_X, train_Y, dev_Y = train_test_split(train_X, train_Y, test_size = TEST_SIZE / (1 - TEST_SIZE), shuffle = True, stratify = train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "# DEFINE MODEL - multilayer sequential model with\n",
    "# regularization to avoid overfitting on individual words,\n",
    "# final activation function should be a softmax activation\n",
    "# where the number of nodes corresponds to the number of \n",
    "# distinct major categories\n",
    "##########################################\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1152, input_dim = train_X.shape[1], activation = 'relu', kernel_regularizer = tf.keras.regularizers.l1(lam)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(576, activation = 'relu', kernel_regularizer = tf.keras.regularizers.l1(LAM)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(288, activation = 'relu', kernel_regularizer = tf.keras.regularizers.l1(LAM)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(144, activation = 'relu', kernel_regularizer = tf.keras.regularizers.l1(LAM)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(72, activation = 'relu', kernel_regularizer = tf.keras.regularizers.l1(LAM)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(72, activation = 'relu', kernel_regularizer = tf.keras.regularizers.l1(LAM)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(36, activation = 'softmax'))\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model defined above, use ADAM optimizer.\n",
    "# Output is categorical, use categorical cross-entropy loss function\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model fit using mini-batch gradient descent\n",
    "test_fit = model.fit(x = train_X, y = train_Y, validation_data = (dev_X, dev_Y), epochs = 40, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "predictions = model.evaluate(x = test_X, y = test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss function",
    "plt.plot(test_fit.history['loss'])\n",
    "plt.plot(test_fit.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'dev'], loc='upper left')\n",
    "plt.ylim((10, 14))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "predictions = model.predict(x = test_X)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert softmax predictions to labels \n",
    "def undo_onehot (df, df_prime):\n",
    "    # Use model to predict output\n",
    "    # out = model.predict(df_prime)\n",
    "    # out = model.predict(df_prime)\n",
    "    out = np.argmax(df_prime, axis = 1)\n",
    "    df = np.argmax(df, axis = 1)\n",
    "\n",
    "    # Inverse transform using the encoder defined above\n",
    "    out = le.inverse_transform(out)\n",
    "    df = le.inverse_transform(df)\n",
    "\n",
    "\n",
    "    # Return as data frame\n",
    "    out = {'y': df,\n",
    "          'y_prime': out,\n",
    "          'match': (df == out) + 0}\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "test_out = undo_onehot(df = test_Y, df_prime = model.predict(test_X))\n",
    "test_out2 = test_out.groupby('y').mean()\n",
    "print(pd.DataFrame.to_latex(test_out2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
